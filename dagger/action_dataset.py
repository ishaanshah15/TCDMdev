from __future__ import print_function

import imageio
import numpy as np
import os
import xml.etree.ElementTree as ET
import torch
import torch.nn
from PIL import Image
import torchvision.transforms as transforms
from torch.utils.data import Dataset
from torch.utils.data import DataLoader




class ActionDatasetRGB(Dataset):
   

    def __init__(self,data_paths):
        super().__init__()
        paths = data_paths 
        
        print(paths)

        self.action_buffers = []
        for path in paths:
            action_buffer = np.load(path,allow_pickle=True)
            
            if type(action_buffer[0]) == list:
                act_buffer = []
                for li in action_buffer:
                    act_buffer += li
                action_buffer = act_buffer
            self.action_buffers.append(action_buffer)
        
        self.action_buffer = np.concatenate(self.action_buffers)
        
        
        self.size = 224


        
        
    def __len__(self):
        return len(self.action_buffer)    


    def __getitem__(self, index):
        """
        :param index: a int generated by Dataloader in range [0, __len__()]
        :return: index-th element
        image: FloatTensor in shape of (C, H, W) in scale [-1, 1].
        label: LongTensor in shape of (Nc, ) binary label
        weight: FloatTensor in shape of (Nc, ) difficult or not.
        """

        
        action_dict = self.action_buffer[index]
        

        
        action = action_dict['policy_action']
        

        
        images = preprocess_frames(action_dict['images'][:4],self.size)
        
        return {'images':images, 'action':action, 'state':action_dict['rl_state']['state'], 'goal':action_dict['rl_state']['goal']}



def split_dataset_pytorch(dataset):
    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
    return train_dataset,test_dataset


def preprocess_frames(frames,size):
    images = []
    for img in frames:

        img = Image.fromarray(img)
            
        trans = transforms.Compose([
            transforms.Resize((size,size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.457, 0.407], std=[0.5, 0.5, 0.5]),
        ])

        img = trans(img)
        images.append(img)
    
    return torch.cat(images)



def preprocess_all(frames,size,state):
    images = []
    for img in frames:

        img = Image.fromarray(img)
            
        trans = transforms.Compose([
            transforms.Resize((size,size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.457, 0.407], std=[0.5, 0.5, 0.5]),
        ])

        img = trans(img)
        images.append(img)
    
    inp_rgb = (torch.cat(images)[None].cuda(),None)
    inp_comb = (torch.cat(images)[None].cuda(),torch.tensor(state['goal'])[None].cuda())
    inp_state = (None,torch.tensor(state['state'])[None].cuda())
    
    return inp_rgb,inp_state,inp_comb





def preprocess(img,size):
    img = Image.fromarray(img)
        
    trans = transforms.Compose([
        transforms.Resize((size,size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.457, 0.407], std=[0.5, 0.5, 0.5]),
    ])

    img = trans(img)
    return img




def get_rgb_dataloader(dataset_path):
    dataset = ActionDatasetRGB(dataset_path)

    
    batch_size = 64

    loader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=4
        )
    return loader



if __name__ == '__main__':
    
    
    dataset = ActVAEDataset(None)
    
    v = dataset.__getitem__(0)
   
    #loader = get_dataloader()
    
